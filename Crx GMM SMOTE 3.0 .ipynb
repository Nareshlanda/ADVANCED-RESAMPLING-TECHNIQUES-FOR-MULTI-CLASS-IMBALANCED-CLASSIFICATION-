{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87812b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from scipy.spatial.distance import cdist\n",
    "import seaborn as sa\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbd19730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load the Dataset\n",
    "df = pd.read_csv('crx.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a98e9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of the dataset:\n",
      "  Unnamed: 0 Unnamed: 1  Unnamed: 2 Unnamed: 3 A1 A2 A3    A4 A5 A6  A7 A8 A9  \\\n",
      "0          b      30.83       0.000          u  g  w  v  1.25  t  t   1  f  g   \n",
      "1          a      58.67       4.460          u  g  q  h  3.04  t  t   6  f  g   \n",
      "2          a       24.5       0.500          u  g  q  h  1.50  t  f   0  f  g   \n",
      "3          b      27.83       1.540          u  g  w  v  3.75  t  t   5  t  g   \n",
      "4          b      20.17       5.625          u  g  w  v  1.71  t  f   0  f  s   \n",
      "\n",
      "   A10  A11 Class  \n",
      "0  202    0     +  \n",
      "1   43  560     +  \n",
      "2  280  824     +  \n",
      "3  100    3     +  \n",
      "4  120    0     +  \n"
     ]
    }
   ],
   "source": [
    "# Step 3: Inspect the Dataset\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "618ac121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Column Names:\n",
      "Index(['Unnamed: 0', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'A1', 'A2',\n",
      "       'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'A11', 'Class'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nColumn Names:\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3654da82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Identify the Target Column (\"Class\")\n",
    "if 'Class' in df.columns:\n",
    "    target_column = 'Class'\n",
    "else:\n",
    "    raise ValueError(\"Class column not found! Please check the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09aefb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Encode categorical variables\n",
    "label_encoders = {}\n",
    "for column in df.columns:\n",
    "    if df[column].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        df[column] = le.fit_transform(df[column])\n",
    "        label_encoders[column] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cd44a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Split Data into Training & Testing Sets\n",
    "X = df.drop(columns=['Class'])\n",
    "y = df['Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "500d42e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Data Preprocessing (Standardization)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b86d6a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class Distribution Before SMOTE:\n",
      "Counter({1: 315, 0: 237})\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Check Class Imbalance\n",
    "print(\"\\nClass Distribution Before SMOTE:\")\n",
    "print(Counter(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6826d66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Apply Gaussian Mixture Model (GMM)\n",
    "k = 5  # Number of clusters\n",
    "gmm = GaussianMixture(n_components=k, random_state=42)\n",
    "clusters = gmm.fit_predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49ae90bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Clusters: [3 3 0 3 3 3 3 3 3 3 3 3 3 0 3 3 3 3 2 0 3 3 3 3 3 0 3 0 3 3 3 0 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0 3 3 3 3 3 3 0 2 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 0 3 3 0 3 3 3 3 3 0 2 3 3 0 3 3 3 3 3 3 3 3 3 0 3 3 3\n",
      " 3 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 0 3 3 3 3 3 3 3 3 3 3 3 3 3 2\n",
      " 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3\n",
      " 3 3 3 3 3 2 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 0 3 3 3 3 3 3 3 3 3\n",
      " 3 0 3 3 3 0 3 3 3 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0 3 3 0 3 3 3 3\n",
      " 3 3 3 3 3 3 3 0 3 3 3 3 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 3 3 3 3 0 3 3 3\n",
      " 3 3 3 3 3 3 3 3 0 3 3 3 3 3 3 3 3 3 3 3 0 3 3 3 0 3 3 3 2 0 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 0 3 3 3 3 3 3 3 3 3 3 3 3 0 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0 3 0 3 3 0 3 0 3 3 4 3 3 3 3\n",
      " 3 0 3 3 3 0 3 3 3 3 3 3 0 3 3 0 3 3 3 3 0 3 3 0 3 3 0 3 3 3 0 0 3 3]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nClusters:\", clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01552301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Filter Clusters with More Minority Instances\n",
    "df_train_clustered = X_train.copy()\n",
    "df_train_clustered['cluster'] = clusters\n",
    "df_train_clustered[target_column] = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe9138d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_clusters = []\n",
    "for c in np.unique(clusters):\n",
    "    cluster_data = df_train_clustered[df_train_clustered['cluster'] == c]\n",
    "    minority_count = cluster_data[target_column].sum()\n",
    "    majority_count = len(cluster_data) - minority_count\n",
    "    imbalance_ratio = (majority_count + 1) / (minority_count + 1)\n",
    "    if imbalance_ratio < 1.5:\n",
    "        filtered_clusters.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8321cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtered Clusters: [0, 3]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFiltered Clusters:\", filtered_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66c16106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11: Compute Sampling Weight for Each Cluster\n",
    "def compute_sampling_weight(cluster_data, target_column, minority_count, X_train):\n",
    "    feature_data = cluster_data.drop(columns=['cluster', target_column]).values\n",
    "    \n",
    "    # Compute pairwise Euclidean distances\n",
    "    pairwise_distances = cdist(feature_data, feature_data, metric='euclidean')\n",
    "    \n",
    "    # Take the mean of the upper triangle (excluding diagonal)\n",
    "    avg_minority_dist = np.mean(pairwise_distances[np.triu_indices(len(feature_data), k=1)])\n",
    "    \n",
    "    print(f\"Cluster {cluster_data['cluster'].iloc[0]} - Avg Euclidean Distance:\", avg_minority_dist)\n",
    "    \n",
    "    density_factor = minority_count / (avg_minority_dist ** X_train.shape[1])\n",
    "    sparsity_factor = 1 / density_factor\n",
    "    \n",
    "    return sparsity_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "719e8b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 - Avg Euclidean Distance: 147.15360505738144\n",
      "Cluster 3 - Avg Euclidean Distance: 965.5334166881864\n"
     ]
    }
   ],
   "source": [
    "sparsity_sum = sum(compute_sampling_weight(\n",
    "    df_train_clustered[df_train_clustered['cluster'] == f], \n",
    "    target_column, \n",
    "    len(df_train_clustered[df_train_clustered['cluster'] == f]), \n",
    "    X_train\n",
    ") for f in filtered_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed162f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 - Avg Euclidean Distance: 147.15360505738144\n",
      "Cluster 3 - Avg Euclidean Distance: 965.5334166881864\n"
     ]
    }
   ],
   "source": [
    "sampling_weights = {\n",
    "    f: compute_sampling_weight(df_train_clustered[df_train_clustered['cluster'] == f], target_column, len(df_train_clustered[df_train_clustered['cluster'] == f]), X_train) / sparsity_sum\n",
    "    for f in filtered_clusters\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3baff9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sampling Weights: {0: 5.5824260482096e-12, 3: 0.9999999999944176}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSampling Weights:\", sampling_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90bc35e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12: Apply SMOTE for Oversampling\n",
    "X_resampled = pd.DataFrame()\n",
    "y_resampled = pd.Series(dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a83f1a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster_id in filtered_clusters:\n",
    "    cluster_data = df_train_clustered[df_train_clustered['cluster'] == cluster_id]\n",
    "    X_cluster = cluster_data.drop(columns=['cluster', target_column])\n",
    "    y_cluster = cluster_data[target_column]\n",
    "    \n",
    "    num_samples = int(len(y_train) * sampling_weights[cluster_id])\n",
    "    \n",
    "    smote = SMOTE(k_neighbors=min(3, len(y_cluster) - 1), random_state=42)\n",
    "    X_cluster_resampled, y_cluster_resampled = smote.fit_resample(X_cluster, y_cluster)\n",
    "    \n",
    "    X_resampled = pd.concat([X_resampled, X_cluster_resampled])\n",
    "    y_resampled = pd.concat([y_resampled, y_cluster_resampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e45d47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class Distribution After SMOTE:\n",
      "Counter({1: 315, 0: 315})\n"
     ]
    }
   ],
   "source": [
    "# Step 13: Check Class Distribution After SMOTE\n",
    "print(\"\\nClass Distribution After SMOTE:\")\n",
    "print(Counter(y_resampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6da9d3d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 14: Train a Classification Model\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18cf6fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 15: Make Predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af116540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 16: Compute Accuracy and ROC AUC Score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "451e3bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.8478\n",
      "ROC AUC Score: 0.9147\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "print(f\"ROC AUC Score: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "157f3a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.7754\n",
      "ROC AUC Score: 0.8311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mahes\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Step 15: Make Predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Step 16: Compute Accuracy and ROC AUC Score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "print(f\"ROC AUC Score: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bba890d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.8478\n",
      "ROC AUC Score: 0.9147\n",
      "Geometric Mean: 0.8479\n",
      "F1 Score: 0.8467\n",
      "PR AUC Score: 0.8265\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, roc_auc_score, f1_score, precision_recall_curve, auc\n",
    ")\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "\n",
    "# Step 13: Train a Classification Model\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_resampled, y_resampled)  # Train on resampled data\n",
    "\n",
    "# Step 14: Make Predictions\n",
    "y_pred = clf.predict(X_test)  # Predicted labels\n",
    "y_proba = clf.predict_proba(X_test)[:, 1]  # Probability of positive class\n",
    "\n",
    "# Step 15: Compute Accuracy and ROC AUC Score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "# Compute Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "g_mean = geometric_mean_score(y_test, y_pred)  # Geometric Mean\n",
    "f1 = f1_score(y_test, y_pred)  # F1-score\n",
    "\n",
    "# Compute Precision-Recall Curve and PR AUC\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
    "pr_auc = auc(recall, precision)  # Area Under Precision-Recall Curve\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "print(f\"Geometric Mean: {g_mean:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"PR AUC Score: {pr_auc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
